#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Apr  7 16:03:57 2019

@author: Selina S. Solomon
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Apr  2 20:07:05 2019

@author: Selina S. Solomon
"""

# Selina 7 April 2019

# Program to choose the optimal # of predictors to predict HDB resale prices

# Import relevant modules
import os
import numpy as np
import pandas as pd
# Plotting
import matplotlib.pyplot as plt
#from mpldatacursor import datacursor
# Model fitting
import statsmodels.api as stats
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, LassoCV, Lasso
from sklearn.model_selection import train_test_split


# Set directories, filenames and url api
curr_dir = os.getcwd()
dataFolder=os.path.join(curr_dir) 
    # This is where I saved the individual .csv files
inputFileName='All_Resale_FlatPrices_Processed.csv'
#cpiFileName='CPI.csv'
    # This is my filename with merged data and geodata
input_file=os.path.join(dataFolder,inputFileName)

# Read in data
data=pd.read_csv(input_file)

# Ok, now let's see which are the best predictors!
quantitative_predictors=['floor_area_sqm','lease_commence_date','prevmonth_price','distance_from_CentralArea',\
                         'distanceOverfloor_area','distanceOverlease_commence_date','resale_year','CPI','n_houses_for_sale']
categorical_predictors=['flat_model','flat_type','town','storey_range'] 
        
all_predictors=quantitative_predictors+categorical_predictors;
all_predictors=np.array(all_predictors)

# So, we want to do an analysis for each year separately, to figure out # optimal predictors for each year
idx=data.loc[:,'resale_year']==1990
data=data.loc[~idx,:]
unique_year=data.loc[:,'resale_year'].unique()
unique_year.sort() # Sort
alpha_vals=[0.001,0.01,0.1,1,2,4,8,10,15,20,50] #

nPredictors=[]
bestPredictors=[] # List of lists
bestModel_R2=[]

for yearcount, yearvalue in enumerate(unique_year):
    idx=data.loc[:,'resale_year']==yearvalue
    y=data.loc[idx,'resale_price']
    single_R2=[]
    
    print('Estimating best # of predictors for year',yearvalue)
    for firstpredictorcount, firstpredictorvalue in enumerate(all_predictors):
        
        if firstpredictorvalue in categorical_predictors:
            X=pd.get_dummies(data.loc[idx,firstpredictorvalue]) # Use dummy variables
            
            # Do separately because we will have to reshape for quantitative predictor 
            # We will split our data into 3 groups:
            # 1. Training set, validation set (to select best parameters of model), & testing set
            # Split into 60:20:20
            X_train, X_test_all, y_train, y_test_all = train_test_split(X, y, test_size=0.4)
                # We have training set
            X_xtest, X_validate, y_xtest, y_validate = train_test_split(X_test_all, y_test_all, test_size=0.5)
        
            # Ok, set up parameters for model selection
            #alpha_vals=np.linspace(0.0001,10,500)
            # Choose best alpha ...
            ridgeCV_model=RidgeCV(alphas=alpha_vals,fit_intercept=True)
            # Now train on validating data and get the appropriate alpha
            ridgeCV_model.fit(X_validate, y_validate)
            # Ok, now fit the model on training data with the selected alpha
            #simpleLM=Ridge(alpha=0.0000000001, fit_intercept=True); # Debug
            simpleLM=Ridge(alpha=ridgeCV_model.alpha_, fit_intercept=True);
    
            simpleLM.fit(X_train,y_train)                  
    
            # Now cross-validate
            temp_r2=simpleLM.score(X_xtest, y_xtest)

        else:
            X=data.loc[idx,firstpredictorvalue]
            X=X.fillna(value=X.mean())
            X=np.array(X)
            X.reshape(-1,1)
        
            # We will split our data into 3 groups:
            # 1. Training set, validation set (to select best parameters of model), & testing set
            # Split into 60:20:20
            X_train, X_test_all, y_train, y_test_all = train_test_split(X, y, test_size=0.4)
                # We have training set
            X_xtest, X_validate, y_xtest, y_validate = train_test_split(X_test_all, y_test_all, test_size=0.5)
        
            # Ok, set up parameters for model selection
            #alpha_vals=np.linspace(0.0001,10,500)
            # Choose best alpha ...
            ridgeCV_model=RidgeCV(alphas=alpha_vals,fit_intercept=True)
            # Now train on validating data and get the appropriate alpha
            ridgeCV_model.fit(X_validate.reshape(-1,1), y_validate)
            # Ok, now fit the model on training data with the selected alpha
            #simpleLM=Ridge(alpha=0.0000000001, fit_intercept=True); # Debug
            simpleLM=Ridge(alpha=ridgeCV_model.alpha_, fit_intercept=True);
    
            simpleLM.fit(X_train.reshape(-1,1),y_train)                  
    
            # Now cross-validate
            temp_r2=simpleLM.score(X_xtest.reshape(-1,1), y_xtest)
        
        # Save data
        single_R2.append(temp_r2)
    
    # Ok.. so now choose the best predictor
    sorti=np.argsort(single_R2)
    reverse_sort=list(reversed(sorti))

    
    # Ok..how about now we fix each predictor, then try to add the second predictor and see which gives s the best R2
    # And then keep going
    leftover_predictors=all_predictors[reverse_sort]
    # Remove ordinals that were not that informative anyway
    lr_predictors=[]
    lr_predictors.append(leftover_predictors[0])
    best_R2=[];
    best_R2.append(single_R2[reverse_sort[0]])

    # Start off with our first predictor, which we know is prevmonth_price
    # drop the first year
    # Don't include the first year since we won't have measurements for previous month + year
    
    if all_predictors[reverse_sort[0]] in categorical_predictors:
        x1=pd.get_dummies(data.loc[idx,all_predictors[reverse_sort[0]]])
    else:
        x1=data.loc[idx,all_predictors[reverse_sort[0]]]
        
    leftover_predictors=np.delete(leftover_predictors,np.where(leftover_predictors==all_predictors[reverse_sort[0]]))
    y=data.loc[idx,'resale_price'];
    old_value=all_predictors[reverse_sort[0]]

    while leftover_predictors.size>0: # While we still have predictors
        temp_r2=[] # Keep track of score of combining with all the other predictors
        print('Predictors to go:',leftover_predictors.size)
        for leftovercount,leftovervalue in enumerate(leftover_predictors):
            #this_predictor=leftover_predictors[0]
            if leftovervalue in categorical_predictors:
                x2=pd.get_dummies(data.loc[idx,leftovervalue]) # Use dummy variables
            else:
                x2=data.loc[idx,leftovervalue]
                x2=x2.fillna(value=x2.mean())
    
            #imp=SimpleImputer(strategy='mean')
            X=pd.concat([x1,x2],axis=1)
            X=X.fillna(value=X.mean())
    
            
            # We will split our data into 3 groups:
            # 1. Training set, validation set (to select best parameters of model), & testing set
            # Split into 60:20:20
            X_train, X_test_all, y_train, y_test_all = train_test_split(X, y, test_size=0.4)
                # We have training set
            X_xtest, X_validate, y_xtest, y_validate = train_test_split(X_test_all, y_test_all, test_size=0.5)

            # Choose best alpha ...
            ridgeCV_model=RidgeCV(alphas=alpha_vals,fit_intercept=True)
            # Now train on validating data and get the appropriate alpha
            ridgeCV_model.fit(X_validate, y_validate)
            # Ok, now fit the model on training data with the selected alpha
            #simpleLM=Ridge(alpha=0.0000000001, fit_intercept=True); # Debug
            simpleLM=Ridge(alpha=ridgeCV_model.alpha_, fit_intercept=True);
            simpleLM.fit(X_train,y_train)                  
        
            # Now cross-validate
            #y_pred = simpleLM.predict(X_xtest.reshape(-1,1))
            #lm_mse=mean_squared_error(y_pred,y_xtest)
            this_r2=simpleLM.score(X_xtest, y_xtest)
            
            #print('\nPredictor: ',value,'+',old_value)
            #print('\nR squared: %.4f' % temp_r2)
            # Save data
            temp_r2.append(this_r2)
    
        # Ok, which was the best predictor?
        temp_r2=np.array(temp_r2)
        tempi=temp_r2.argmax()
        temp_maxR2=temp_r2.max()
    
        # Update values
        best_R2.append(temp_r2[tempi])
        lr_predictors.append(leftover_predictors[tempi])
        x1=X
        leftover_predictors=np.delete(leftover_predictors,np.where(leftover_predictors==leftover_predictors[tempi]))
        
    # Ok...so now let's see what was the best predictor for this year...
    tempn=np.argmax(best_R2)+1 # Add one because index removes one
    nPredictors.append(tempn)
    
    # And what were those predictors?
    bestPredictors.append(lr_predictors[0:tempn])
    bestModel_R2.append(best_R2[tempn])
    
# Plot a histogram of the predictors
plt.figure(figsize=(10,12))
# Count number of times a predictor is important
count_predictorContribution=[]
flattened_bestPredictors=[]

for x in bestPredictors:
    for y in x:
        flattened_bestPredictors.append(y)
        


for count,value in enumerate(all_predictors):
    tempn=flattened_bestPredictors.count(value)/len(flattened_bestPredictors)
    count_predictorContribution.append(tempn)

# Sort it
sorti=np.argsort(count_predictorContribution)
reverse_sort=list(reversed(sorti))
count_predictorContribution=np.array(count_predictorContribution)
# Now plot
plt.figure(figsize=(6,8))
ax=plt.subplot()
plt.bar(['Town','Flat type','Flat size'],count_predictorContribution[reverse_sort[0:3]],color='blueviolet')
#plt.bar(all_predictors[reverse_sort[0:3]],count_predictorContribution[reverse_sort[0:3]],color='indigo')
plt.ylabel('Frequency',fontsize=20)
plt.xticks(fontsize=15,rotation=90)
plt.yticks([0,0.2,0.4],fontsize=15)
plt.ylim([0,0.4])
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.gcf().subplots_adjust(bottom=0.4,left=0.2)